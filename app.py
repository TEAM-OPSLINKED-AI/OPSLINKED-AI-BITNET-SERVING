# ============================================================
# app_fast.py ‚Äî Optimized FastAPI RAG Server for AIOps Llama3
# ============================================================

import logging
import time
import os
from typing import List, Dict

from fastapi import FastAPI, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from motor.motor_asyncio import AsyncIOMotorClient
from dotenv import load_dotenv

# ============================================================
# Ï¥àÍ∏∞ ÌôòÍ≤Ω ÏÑ§Ï†ï
# ============================================================
load_dotenv()

# Hugging Face Ï∫êÏãú Í≤ΩÎ°ú ÏßÄÏ†ï (ÏÜçÎèÑ Ìñ•ÏÉÅ)
os.environ["TRANSFORMERS_CACHE"] = "./hf_cache"
os.makedirs("./hf_cache", exist_ok=True)

logger = logging.getLogger("uvicorn.error")

# ============================================================
# ÌôòÍ≤Ω Î≥ÄÏàò / DB ÏÑ§Ï†ï
# ============================================================
HF_TOKEN = os.environ.get("HF_TOKEN")
if not HF_TOKEN:
    logger.warning("‚ö†Ô∏è HF_TOKEN environment variable not found ‚Äî gated model access may fail.")

MONGO_URI = "mongodb://root:NYdrCjppRgNRdatI@121.138.215.117:27017/?authSource=admin"
mongo_client = AsyncIOMotorClient(MONGO_URI)
db = mongo_client["metrics_db"]
mysql_col = db["mysql_metrics"]
node_col = db["node_metrics"]

# ============================================================
# Î™®Îç∏ ÏÑ§Ï†ï
# ============================================================
BASE_MODEL_NAME = "meta-llama/Meta-Llama-3-8B"
ADAPTER_MODEL_NAME = "DKCode9/AIOps-peft-Llama3-8B-v1"
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-small"
SCENARIO_FILE_PATH = "aiops_scenarios.txt"

# Ï†ÑÏó≠ Í∞ùÏ≤¥
tokenizer = None
model = None
embedding_model = None
scenario_texts = []
scenario_embeddings = None

# ============================================================
# Alpaca ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø
# ============================================================
ALPACA_PROMPT_TEMPLATE = """Below is an instruction that describes a task, paired with an input that provides further context.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

STATIC_INSTRUCTION = (
    "You are an AI SRE specializing in Kubernetes infrastructure. "
    "Analyze why this situation occurred, describe automated actions, "
    "and identify the corresponding scenario and remediation."
)

# ============================================================
# Î™®Îç∏ Î°úÎìú Ìï®Ïàò
# ============================================================
def load_models():
    global tokenizer, model, embedding_model, scenario_texts, scenario_embeddings

    logger.info("üöÄ Loading all models and RAG data...")
    start_time = time.time()

    # 4bit ÏñëÏûêÌôî ÏÑ§Ï†ï
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        llm_int8_enable_fp32_cpu_offload=True
    )

    # Base Llama Î™®Îç∏ Î°úÎìú
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        token=HF_TOKEN,
        trust_remote_code=True,
        quantization_config=quant_config
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, token=HF_TOKEN)
    logger.info(f"‚úÖ Base model loaded: {BASE_MODEL_NAME}")

    # LoRA Ïñ¥ÎåëÌÑ∞ Î°úÎìú (Î≥ëÌï© Ïïà Ìï®)
    model = PeftModel.from_pretrained(base_model, ADAPTER_MODEL_NAME, token=HF_TOKEN)
    logger.info(f"‚úÖ LoRA adapter loaded: {ADAPTER_MODEL_NAME}")

    # ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device="cpu")
    logger.info(f"‚úÖ Embedding model loaded: {EMBEDDING_MODEL_NAME}")

    # RAG ÌååÏùº Î°úÎìú
    if not os.path.exists(SCENARIO_FILE_PATH):
        logger.error(f"‚ùå Scenario file not found: {SCENARIO_FILE_PATH}")
    else:
        with open(SCENARIO_FILE_PATH, "r", encoding="utf-8") as f:
            scenario_texts[:] = [line.strip() for line in f if line.strip()]
        scenario_embeddings = embedding_model.encode(scenario_texts, convert_to_tensor=False)
        logger.info(f"‚úÖ Loaded and encoded {len(scenario_texts)} scenarios.")

    logger.info(f"‚úÖ Total load time: {time.time() - start_time:.2f}s")

# ============================================================
# RAG Í≤ÄÏÉâ
# ============================================================
def retrieve_context(query: str, top_k: int = 3):
    if not scenario_embeddings or not scenario_texts:
        return "No context available.", 0

    query_vec = embedding_model.encode([query], convert_to_tensor=False)
    similarities = cosine_similarity(query_vec, scenario_embeddings)
    top_indices = np.argsort(similarities[0])[-top_k:][::-1]
    retrieved_docs = [scenario_texts[i] for i in top_indices]
    return "\n".join(retrieved_docs), len(scenario_texts)

# ============================================================
# FastAPI Ï¥àÍ∏∞Ìôî
# ============================================================
app = FastAPI()

@app.on_event("startup")
def on_startup():
    load_models()

# ============================================================
# ÏöîÏ≤≠ Î™®Îç∏
# ============================================================
class GenerationRequest(BaseModel):
    prompt: str

# ============================================================
# ÏóîÎìúÌè¨Ïù∏Ìä∏
# ============================================================
@app.post("/generate")
def generate_text(request: GenerationRequest):
    if not all([model, tokenizer, embedding_model]):
        return JSONResponse({"error": "Models not ready"}, status_code=500)

    try:
        start = time.time()
        context, _ = retrieve_context(request.prompt)
        prompt = ALPACA_PROMPT_TEMPLATE.format(
            STATIC_INSTRUCTION,
            f"Problem:\n{request.prompt}\n\nContext:\n{context}",
            ""
        )
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # Ïä§Ìä∏Î¶¨Î∞ç Ï†úÍ±∞ ‚Üí Ï¶âÏãú Í≤∞Í≥º ÏÉùÏÑ±
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True,
            use_cache=True
        )

        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"Generated in {time.time() - start:.2f}s")

        return {"response": text.strip()}

    except Exception as e:
        logger.error(f"Error during generation: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)

# ============================================================
# Health check
# ============================================================
@app.get("/health")
def health_check():
    ok = all([model, tokenizer, embedding_model, scenario_embeddings is not None])
    return {"status": "ok" if ok else "error"}

# ============================================================
# ÎπÑÎèôÍ∏∞ MongoDB ÏòàÏãú
# ============================================================
class MetricDocumentResponse(BaseModel):
    id: str
    metricName: str
    labels: Dict[str, str]
    value: float
    timestamp: int


@app.get("/metrics/node/filesystem_free", response_model=List[MetricDocumentResponse])
async def get_node_metrics(
    mountpoint: str = Query(...),
    limit: int = Query(10)
):
    query = {"metricName": "node_filesystem_free_bytes", "labels.mountpoint": mountpoint}
    docs = await node_col.find(query).sort("timestamp", -1).to_list(limit)
    return [
        MetricDocumentResponse(
            id=str(doc["_id"]),
            metricName=doc["metricName"],
            labels=doc["labels"],
            value=doc["value"],
            timestamp=doc["timestamp"]
        )
        for doc in docs
    ]

@app.get("/metrics/mysql/commands_total", response_model=List[MetricDocumentResponse])
async def get_mysql_metrics(
    command: str = Query(...),
    limit: int = Query(10)
):
    query = {"metricName": "mysql_global_status_commands_total", "labels.command": command}
    docs = await mysql_col.find(query).sort("timestamp", -1).to_list(limit)
    return [
        MetricDocumentResponse(
            id=str(doc["_id"]),
            metricName=doc["metricName"],
            labels=doc["labels"],
            value=doc["value"],
            timestamp=doc["timestamp"]
        )
        for doc in docs
    ]
